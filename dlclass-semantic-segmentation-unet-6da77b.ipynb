{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as tfl\nimport os\nfrom tqdm import tqdm\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom PIL import Image\nfrom os.path import splitext\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"pMItZeDA6lQU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Loading Data & Data Preprocessing</b>\n\n<p style=\"padding: 10px;color:black;font-size:40%;font-family: Times New Roman, Times, serif;\">First using the OS library I'll upload the cityscapes pair images.</p>","metadata":{}},{"cell_type":"code","source":"os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\npath='../input/cityscapes-image-pairs'\ndef images_upload(path):\n    images=[]\n    for root,subfolders,files in os.walk(path):\n        for file in tqdm(files):\n            filename=root+os.sep+file\n            if filename.endswith('jpg') or filename.endwith('png'):\n                images.append(filename)\n    return images\nimages=images_upload(path)","metadata":{"id":"1c89dneE6lQc","outputId":"029850c8-d5f3-4022-e903-d221d2d8df10","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_image_rgb(data):\n    imgs=[]\n    for i in tqdm(data):\n        img = cv2.imread(i,cv2.COLOR_BGR2RGB)\n        del i\n        imgs.append(img)\n    return imgs\nimg=convert_image_rgb(images)","metadata":{"id":"U43yHpet6lQd","outputId":"6b0e6669-0dd1-4ca3-9b84-8b5cfa8f44d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"padding: 10px;color:black;font-size:120%;font-family: Times New Roman, Times, serif;\">Let us look at one of the images. It can be seen that the image contains both the real image and the mask, so the next task will be to distinguish between them. Randmoly has selected the following image.</p>","metadata":{}},{"cell_type":"code","source":"plt.imshow(img[np.random.randint(0,len(img))]);","metadata":{"id":"QT2AijwsPS0j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"padding: 10px;color:black;font-size:120%;font-family: Times New Roman, Times, serif;\">Because the image shape is 512*256, it should be divided into two groups, real images and mask images, at 256 pixels on the x axis.</p>","metadata":{}},{"cell_type":"code","source":"def split_input_mask(data):\n    inputs=[]\n    mask=[]\n    for i in data:\n        a=i[:,:256]\n        inputs.append(a)\n        b=i[:,256:]\n        mask.append(b)\n    return inputs,mask\ninputs,mask=split_input_mask(img)","metadata":{"id":"3m7Peda-6lQe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del images\ndel img","metadata":{"id":"u_17--7Wx6sn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"padding: 10px;color:black;font-size:120%;font-family: Times New Roman, Times, serif;\">In this section, I'll show a few images of the real images as well as a few images of the mask images after they've been separated.</p>","metadata":{}},{"cell_type":"code","source":"def show_images(data):\n    plt.figure(figsize=(10,10))\n    for i in range(9):\n        idx=np.random.randint(0,len(data))\n        plt.subplot(3,3,i+1)\n        img=data[idx]\n        plt.imshow(img)\nshow_images(inputs)","metadata":{"id":"M-4aI85vP39V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_images(mask)","metadata":{"id":"EwAKE3ubQB90","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def images_compare(inputs,mask):\n    idx_new=np.random.randint(0,len(mask))\n    fig = plt.figure()\n    ax1 = fig.add_subplot(1,2,1)\n    ax1.imshow(mask[idx_new])\n    ax2 = fig.add_subplot(1,2,2)\n    ax2.imshow(inputs[idx_new],cmap='gray')\n    plt.show()\nimages_compare(inputs,mask)","metadata":{"id":"BK4uLieXQHz8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Labeling using K-means</b>\n\n<p style=\"padding: 10px;color:black;font-size:40%;font-family: Times New Roman, Times, serif;\">Although each pixel in these images is labeled, it was still necessary to separate the different colors to labels. K-means will be used to categorize these colored labels into ten different groups. This is a hyperparameter. The next few labeled images will be shown, followed by rescaling of the original image.</p>","metadata":{}},{"cell_type":"code","source":"num_items = 1000\ncolor_array = np.random.choice(range(256), 3*num_items).reshape(-1,3)\nnum_classes = 20\nlabel_model = KMeans(n_clusters = num_classes)\nlabel_model.fit(color_array)\nlabel_class = label_model.predict(mask[10].reshape(-1,3)).reshape(256,256)\nfig, axes = plt.subplots(1,3,figsize=(15,5))\naxes[0].imshow(inputs[10]);\naxes[1].imshow(mask[10]);\naxes[2].imshow(label_class);","metadata":{"id":"JsJMO7yay5G-","outputId":"b7a8b56a-6223-458e-a15a-32bf5ae2dac4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def new_labels(mask):\n    num_items = 1000\n    color_array = np.random.choice(range(256), 3*num_items).reshape(-1,3)\n    num_classes = 10\n    label_model = KMeans(n_clusters = num_classes)\n    label_model.fit(color_array)\n    labels=[]\n    for i in tqdm(range(len(mask))):\n        label_class = label_model.predict(mask[i].reshape(-1,3)).reshape(256,256)\n        labels.append(label_class)\n    return labels\n","metadata":{"id":"HYgHgUw74OJB","outputId":"d5f26e52-7858-4d0b-b75c-7681da066e45","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=new_labels(mask)\nidx=np.random.randint(0,len(labels))\nclasses,freq=np.unique(labels[idx],return_counts=True)\nprint(f'number of classes :{len(classes)}')","metadata":{"id":"xZk8TG1X0YcA","outputId":"5780fe55-cdde-4e91-a188-b5b35323547d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nfor i in range(9):\n        plt.subplot(3,3,i+1)\n        im=labels[i]\n        plt.imshow(im)\n# plt.imshow(a)","metadata":{"id":"FMLWOnO_hwY0","outputId":"dfaf45a2-a324-45c2-830c-a0764f59e185","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rescale(data):\n    rescaled=[]\n    for i in tqdm(data):\n        img=tf.image.convert_image_dtype(i, tf.float32)\n        del i\n        rescaled.append(img)\n    return rescaled","metadata":{"id":"ewpnBM846lQp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rescaled_input=rescale(inputs)\n","metadata":{"id":"AQgZQpZT6lQt","outputId":"85ff875d-e8d6-42fd-c1cd-90b1e3b11607","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Buliding the model</b>\n\n<p style=\"padding: 10px;color:black;font-size:40%;font-family: Times New Roman, Times, serif;\"> Here I'm using UNet since the position of the object is crucial for this task. The feature map is upsampled to the size of the original input image using a transposed convolution layer that preserves the spatial information. It also includes skip connections, which help to keep information that would otherwise be lost during encoding. This model is kind of mini-VGG. </p>","metadata":{}},{"cell_type":"code","source":"def build_model(inputsize=(256,256,3),classes=None):\n    inputs = tf.keras.Input(shape=(inputsize))\n\n    conv = tfl.Conv2D(32, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv1')(\n        inputs)\n    x=tfl.BatchNormalization()(conv)\n    x=tfl.LeakyReLU()(x)\n    x1 = tfl.Conv2D(32, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv2')(\n        x)\n    x=tfl.BatchNormalization()(x1)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.MaxPool2D(pool_size=(2, 2), strides=(2, 2), name='MaxPool1')(x)\n\n    x = tfl.Conv2D(64, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv3')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x2 = tfl.Conv2D(64, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv4')(x)\n    x=tfl.BatchNormalization()(x2)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.MaxPool2D(pool_size=(2, 2), name='MaxPool2')(x)\n\n    x = tfl.Conv2D(128, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv5')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x3 = tfl.Conv2D(128, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv6')(x)\n    x=tfl.BatchNormalization()(x3)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.MaxPool2D(pool_size=(2, 2), strides=(2, 2), name='MaxPool3')(x)\n\n    \n    x = tfl.Conv2D(256, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv7')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(256, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv8')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    \n    x = tfl.Conv2DTranspose(128, (3, 3), strides=2, padding=\"same\")(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n\n    x = tfl.concatenate([x, x3], axis=3)\n    \n    x = tfl.Conv2D(128, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv9')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(128, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv10')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2DTranspose(64, (3, 3), strides=2, padding=\"same\")(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n\n    x = tfl.concatenate([x, x2], axis=3)\n\n    x = tfl.Conv2D(64, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv11')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(64, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv12')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    \n    x = tfl.Conv2DTranspose(32, (3, 3), strides=2, padding=\"same\")(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n\n    x = tfl.concatenate([x, x1], axis=3)\n\n    x = tfl.Conv2D(32, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv25')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(32, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv26')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    \n    outputs = tfl.Conv2D(classes, (1, 1), padding=\"same\", activation='softmax', name='Outputs')(x)\n    final_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    final_model.summary()\n    return final_model","metadata":{"id":"TzJafsen6lQt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mymodel=build_model(classes=10)","metadata":{"id":"_puX0qFc6lQu","outputId":"e8ae46b6-e887-41dd-d0c2-1dcbc16ecae1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_file = './model_arch.png'\n\ntf.keras.utils.plot_model(mymodel, to_file=img_file, show_shapes=True, show_layer_names=True)","metadata":{"id":"9neiNSL56lQu","outputId":"495c1522-5683-4494-9d13-6a9d1ea1d32e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del inputs","metadata":{"id":"fg9sE6VJ-g6d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Training the model</b>\n\n<p style=\"padding: 10px;color:black;font-size:40%;font-family: Times New Roman, Times, serif;\"> The data is divided into train and test segments, with each segment accounting for 80% and 20% of the total. To prevent RAM memory from crashing, I'm using 1300 images. Because the model's output employs 'softmax' activation layer, the loss function is sparse categorical crossentropy. Next, I'm going to use argmax to return the index with the highest value, and then I'm going to make some predictions for the test.</p>","metadata":{}},{"cell_type":"code","source":"def split_data(x,y,test_size=0.2):\n    x1=np.array(x)\n    del x\n    y1=np.array(y)\n    del y\n    x_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=test_size)\n    return  x_train, x_test, y_train, y_test","metadata":{"id":"dqE4v5MuoHrF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" x_train, x_test, y_train, y_test=split_data(rescaled_input[:1300],labels[:1300],test_size=0.2)\n\n  ","metadata":{"id":"jSTAK6mPoKrv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def callbacks(patience=5):\n    checkpoint = tf.keras.callbacks.ModelCheckpoint('seg_model.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True)\n    early=tf.keras.callbacks.EarlyStopping(monitor='loss',patience=patience)\n    callbacks_list=[checkpoint, early]\n    return callbacks_list\n","metadata":{"id":"tgwn1RJVpE8K","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mymodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),loss=tf.keras.losses.sparse_categorical_crossentropy,metrics=['acc'])\nhist=mymodel.fit(x_train,y_train,batch_size=16,epochs=200,callbacks=callbacks())","metadata":{"id":"Azc5fqAJoBmz","outputId":"a80717ce-5364-49e3-9256-d8f86aac8aeb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(hist.history['loss'])\nplt.title(\"model loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.show()","metadata":{"id":"eeGX2qLSiwxR","outputId":"ae53624c-a80f-49bc-ba4f-ace3441503a6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(hist.history[\"acc\"])\nplt.title(\"model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.show()","metadata":{"id":"wPriUOjAi157","outputId":"bd1fa575-a4f0-45f8-be0b-8e557c5b98ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=mymodel.predict(x_test)\ny_pred=tf.argmax(pred,axis=-1)","metadata":{"id":"5XWEFJb6afCN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef show_predications(x_test,y_test,y_pred):\n    idx=np.random.randint(0,len(y_pred))\n    fig, axes = plt.subplots(1,3,figsize=(10,10))\n    axes[0].imshow(x_test[idx])\n    axes[0].set_title(\"original\")\n    axes[1].imshow(y_test[idx])\n    axes[1].set_title(\"mask\")\n    axes[2].imshow(y_pred[idx])\n    axes[2].set_title(\"predicated\")\n \n","metadata":{"id":"OVU13MSXA6-4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    show_predications(x_test,y_test,y_pred)","metadata":{"id":"Dw78GG7RE_0P","outputId":"0702ffff-ff18-47eb-ba5c-de28710dd69f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Conclusions</b>\n\n<p style=\"padding: 10px;color:black;font-size:40%;font-family: Times New Roman, Times, serif;\">\nDespite the fact that the results are not amazing, they are not so bad. More layers, as well as augmentations and different class clustering, are required for even better results.</p>","metadata":{}}]}